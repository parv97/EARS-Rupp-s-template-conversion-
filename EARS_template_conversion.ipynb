{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(string):\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    matches = tool.check(string)\n",
    "    return language_check.correct(string, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from textacy import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_list_string(lst):\n",
    "    for i in range(len(lst)):\n",
    "        print(i+1,\":\",lst[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_past_sentence(sentence):\n",
    "    sent = list(nlp(sentence).sents)[0]\n",
    "    return (\n",
    "        sent.root.tag_ == \"VBD\" or\n",
    "        any(w.dep_ == \"aux\" and w.tag_ == \"VBD\" for w in sent.root.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_passive_voice(sentence):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    doc = nlp(sentence)    \n",
    "    passive_rule = [{'DEP':'nsubjpass'},{'DEP':'aux','OP':'*'},{'DEP':'auxpass'},{'TAG':'VBN'}]\n",
    "    matcher.add('Passive',None,passive_rule)\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_actors(desc):\n",
    "    doc1 = nlp(desc)\n",
    "    actor = []\n",
    "    for sent in doc1.sents:\n",
    "        subj = \"\"\n",
    "        SBOV_ext = textacy.extract.subject_verb_object_triples(sent)\n",
    "        SBOV = list(SBOV_ext)\n",
    "        if len(SBOV) != 0:\n",
    "            subj = SBOV[0][0].text\n",
    "\n",
    "        for token in sent:\n",
    "            #--------------------------------actor identification--------------------------------\n",
    "            if token.tag_ == \"NN\" and token.dep_ == \"nsubj\" and subj == token.text:\n",
    "                if token.text.lower() not in actor:\n",
    "                    actor.append(token.text.lower())\n",
    "    return actor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_nsubj(sent):\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            return token.text.lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EARS_template_1(sent,EARS_template_1):         #WHEN ..system shall...\n",
    "    temp = list.copy(EARS_template_1)\n",
    "    \n",
    "    comma_idx = sent.text.find(\",\")\n",
    "    \n",
    "    temp[1] = sent.text[5:comma_idx]\n",
    "    \n",
    "    comma_flg = False\n",
    "    rem_str = \"\"\n",
    "    verb_flg = False\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.tag_ == \",\":\n",
    "            comma_flg = True\n",
    "        elif comma_flg:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                temp[3] += token.lemma_\n",
    "                temp[3] += \" \"\n",
    "                verb_flg = True\n",
    "            elif verb_flg:\n",
    "                if token.text.lower() == \"system\":\n",
    "                    continue\n",
    "                else:\n",
    "                    rem_str += token.text\n",
    "                    rem_str += \" \"\n",
    "    \n",
    "    temp[3] += rem_str\n",
    "    \n",
    "    if temp[1] != \"\" and temp[3] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EARS_template_2(sent,EARS_template_2,actor):\n",
    "    temp = list.copy(EARS_template_2)\n",
    "        \n",
    "\n",
    "    then_idx = sent.text.find(\"then\")\n",
    "    then_next = then_idx+5\n",
    "\n",
    "    if then_idx == -1:             #if then not found\n",
    "        then_idx = sent.text.find(\",\")\n",
    "        then_next = then_idx+1\n",
    "\n",
    "    if sent.text.startswith(\"If\"):    \n",
    "        temp[1] = sent.text[3:then_idx]\n",
    "    else:\n",
    "        temp[1] = sent.text[:then_idx]\n",
    "\n",
    "\n",
    "    verb_string = str(\"\")\n",
    "    temp[3] = \"\"\n",
    "    idx_rest_of_sent = 0\n",
    "    idx_aux = 0\n",
    "    flg = False\n",
    "    first_flg = False\n",
    "    \n",
    "    #without then statements remaining\n",
    "    \n",
    "    for i in range(len(sent)):\n",
    "        if sent[i].text == \"then\" and sent[i].dep_ == \"advmod\":\n",
    "            temp[3] += str(sent[i].head.lemma_)\n",
    "            verb_string = sent[i].head.text\n",
    "            flg = True\n",
    "            \n",
    "        elif flg:\n",
    "            if sent[i].pos_.lower() == \"aux\" and first_flg == False:\n",
    "                first_flg = True\n",
    "                \n",
    "            elif sent[i].text == \"system\":\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                if sent[i].text != verb_string:\n",
    "                    if sent[i].tag_ == \"PRP\":\n",
    "                        if actor[0] != \"system\":\n",
    "                            temp[3] += str(actor[0])\n",
    "                            temp[3] += \" \"\n",
    "                        else:\n",
    "                            temp[3] += str(actor[1])\n",
    "                            temp[3] += \" \"\n",
    "                    else:\n",
    "                        temp[3] += str(sent[i].text)\n",
    "                        temp[3] +=\" \"\n",
    "        \n",
    "            \n",
    "    if temp[1] == \"\" or temp[3] == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EARS_template_3(sent,EARS_template_3):\n",
    "    temp = list.copy(EARS_template_3)    #while template\n",
    "    \n",
    "    comma_idx = sent.text.find(\",\")\n",
    "    \n",
    "    temp[1] = sent.text[6:comma_idx]\n",
    "    \n",
    "    comma_flg = False\n",
    "    rem_str = \"\"\n",
    "    verb_flg = False\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.tag_ == \",\":\n",
    "            comma_flg = True\n",
    "        elif comma_flg:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                temp[3] += token.lemma_\n",
    "                temp[3] += \" \"\n",
    "                verb_flg = True\n",
    "            elif verb_flg:\n",
    "                if token.text.lower() == \"system\":\n",
    "                    continue\n",
    "                else:\n",
    "                    rem_str += token.text\n",
    "                    rem_str += \" \"\n",
    "    \n",
    "    temp[3] += rem_str\n",
    "    \n",
    "    if temp[1] != \"\" and temp[3] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Rupp_template_2(sent,Rupp_template_2,actor):\n",
    "    temp = list.copy(Rupp_template_2)      #change index 1-customer 3-PROCESS\n",
    "    subj_flg = False\n",
    "    verb_flg = False\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\" and token.text.lower() in actor:\n",
    "            subj_flg = True\n",
    "            temp[1] = token.text\n",
    "        \n",
    "        elif subj_flg == True:\n",
    "            if verb_flg == False and token.pos_.lower() == \"verb\":\n",
    "                verb_flg = True\n",
    "                temp[3] += token.lemma_\n",
    "                temp[3] += \" \"\n",
    "            else:\n",
    "                if token.tag_ == \"PRP\" and (token.text == \"he\" or token.text == \"she\"):\n",
    "                    if actor[0] != \"system\":\n",
    "                        temp[3] += str(actor[0])\n",
    "                        temp[3] += \" \"\n",
    "                    else:\n",
    "                        temp[3] += str(actor[1])\n",
    "                        temp[3] += \" \"\n",
    "                else:\n",
    "                    temp[3] += token.text\n",
    "                    temp[3] += \" \"\n",
    "                \n",
    "    if temp[1] != \"\" and temp[3] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Rupp_template_3(sent,Rupp_template_3,actor):\n",
    "    temp = list.copy(Rupp_template_3)      #change index 1-PROCESS\n",
    "    subj_flg = False\n",
    "    verb_flg = False\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\" and token.text.lower() in actor:\n",
    "            subj_flg = True\n",
    "        \n",
    "        elif subj_flg == True:\n",
    "            if verb_flg == False and token.pos_.lower() == \"verb\":\n",
    "                verb_flg = True\n",
    "                temp[1] += token.lemma_\n",
    "                temp[1] += \" \"\n",
    "            else:\n",
    "                temp[1] += token.text\n",
    "                temp[1] += \" \"\n",
    "    \n",
    "    if temp[1] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_requirements(desc,req):\n",
    "    req = preprocessing.normalize_whitespace(req) #removing extra spaces\n",
    "\n",
    "    doc = nlp(req)\n",
    "\n",
    "    pre_cond = []\n",
    "    actor_action = []\n",
    "    condition = []\n",
    "    response = []\n",
    "\n",
    "    desc = preprocessing.normalize_whitespace(desc)\n",
    "\n",
    "    actor = identify_actors(desc)\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        md_flag = False\n",
    "        admod = False\n",
    "        mark = False\n",
    "        cond = False\n",
    "        sys_flg = False\n",
    "\n",
    "\n",
    "        for token in sent:\n",
    "\n",
    "            #---------------------------------actor action---------------------------------------\n",
    "            if token.text in actor and detect_passive_voice(sent.text) == False and detect_past_sentence(sent.text) == False and sent not in actor_action:\n",
    "                actor_action.append(sent)\n",
    "\n",
    "            #--------------------------------precondition------------------------------------------    \n",
    "            if token.tag_ == \"MD\":\n",
    "                md_flag = True\n",
    "\n",
    "            if md_flag == True and sent not in pre_cond:\n",
    "                if detect_past_sentence(sent.text) == True:\n",
    "                    if token.tag_ == \"VBN\":\n",
    "                        pre_cond.append(sent)\n",
    "                else:\n",
    "                    pre_cond.append(sent)\n",
    "\n",
    "            #------------------------------conditional-----------------------------------------------\n",
    "            if token.dep_ == \"advmod\":\n",
    "                admod = True\n",
    "\n",
    "            if token.dep_ == \"mark\":\n",
    "                mark = True\n",
    "\n",
    "            if cond == False:\n",
    "                if admod == True and mark == True:\n",
    "                    condition.append(sent)\n",
    "                    cond = True\n",
    "\n",
    "            #------------------------------response-------------------------------------------------\n",
    "            if token.text.lower() == \"system\":\n",
    "                sys_flg = True\n",
    "\n",
    "            if sys_flg == True and token.text.lower() in actor and detect_past_sentence(sent.text) == False and sent not in response:\n",
    "                response.append(sent)\n",
    "\n",
    "\n",
    "    print(\"---------------ACTORS--------------------\")\n",
    "    print_list_string(actor)\n",
    "    print(\"---------------PRE-CONDITION--------------------\")\n",
    "    print_list_string(pre_cond)\n",
    "    print(\"---------------CONDITION--------------------\")\n",
    "    print_list_string(condition)\n",
    "    print(\"---------------RESPONSE--------------------\")\n",
    "    print_list_string(response)\n",
    "    print(\"---------------ACTOR_ACTION--------------------\")\n",
    "    print_list_string(actor_action)\n",
    "\n",
    "    #-----------------------------template structures--------------------------------\n",
    "    Rupp_template_1 = [\"The System shall \",\"\"]                               #replace index 1(PROCESS)\n",
    "    Rupp_template_2 = [\"The System shall provide \",\"\",\"the ability to \",\"\"]  #replace index 1(ACTOR) and 3(PROCESS)\n",
    "    Rupp_template_3 = [\"The System shall be able to \",\"\"]                    #replace index 1(PROCESS)\n",
    "\n",
    "    EARS_template_1 = [\"When \",\"\",\"the system shall \",\"\"]                     #replace index 1(PRE-COND||TRIGGER) and 3(SYSTEM RESPONSE)\n",
    "    EARS_template_2 = [\"If \",\"\",\"then the system shall \",\"\"]                  #replace index 1(PRE-COND||TRIGGER) and 3(SYSTEM RESPONSE)\n",
    "    EARS_template_3 = [\"While \",\"\",\"the system shall \",\"\"]                    #replace index 1(SYSTEM STATE) and 3(SYSTEM RESPONSE)\n",
    "    EARS_template_4 = [\"Where \",\"\",\"the system shall \",\"\"]                    #replace index 1(FEATURE) and 3(SYSTEM RESPONSE)\n",
    "\n",
    "    #------------------------second pass-----------------------\n",
    "    results = []\n",
    "    start_char = []\n",
    "    end_char = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        #--------------conditoinal--------------\n",
    "        if sent in condition:\n",
    "            res = make_EARS_template_2(sent,EARS_template_2,actor)\n",
    "            if res != \"\":\n",
    "                results.append(res)\n",
    "                start_char.append(sent.start_char)\n",
    "                end_char.append(sent.end_char)\n",
    "\n",
    "        #--------------actor_actional---------------\n",
    "        elif sent in actor_action:\n",
    "            if identify_nsubj(sent) == \"system\":\n",
    "                res = make_Rupp_template_3(sent,Rupp_template_3,actor) \n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "            else:\n",
    "                res = make_Rupp_template_2(sent,Rupp_template_2,actor)\n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "\n",
    "        else:\n",
    "            if sent.text.startswith(\"When\"):\n",
    "                res = make_EARS_template_1(sent,EARS_template_1)\n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "            elif sent.text.startswith(\"While\"):\n",
    "                res = make_EARS_template_3(sent,EARS_template_3)\n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "\n",
    "    \n",
    "    return results, start_char, end_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Quality-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\parvs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text\n",
    "\n",
    "def sentence_similarity(sent1,sent2):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sent1 = [token.lower() for token in sent1]\n",
    "    sent2 = [token.lower() for token in sent2]\n",
    "    \n",
    "    all_words = list(set(sent1+sent2))\n",
    "    \n",
    "    vector1 = [0]*len(all_words)\n",
    "    vector2 = [0]*len(all_words)\n",
    "    \n",
    "    for w in sent1:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        vector1[all_words.index(w)]+=1\n",
    "    \n",
    "    for w in sent2:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        vector2[all_words.index(w)]+=1\n",
    "    return 1 - cosine_distance(vector1,vector2)\n",
    "    \n",
    "\n",
    "def get_duplication_index(sentences):\n",
    "    count = 0\n",
    "    global len_sent\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i+1,len(sentences)):\n",
    "            if sentence_similarity(sentences[i],sentences[j]) > 0.99:\n",
    "                count+=1\n",
    "                print(\"Number :\",count)\n",
    "                print(\"Sent_index\",i,\":\",sentences[i])\n",
    "                print(\"Sent_index\",j,\":\",sentences[j])\n",
    "                \n",
    "    return count/len_sent\n",
    "#     print(\"Duplication index :\"+\"{:.2f}\".format(count/len(sentences)))\n",
    "\n",
    "def get_wordliness_index(requirement_text,total_sentences):\n",
    "    count = 0\n",
    "    tokens = nltk.word_tokenize(requirement_text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for i in tagged:\n",
    "        if i[1] == 'RB':\n",
    "            if i[0] != 'then' and i[0] != 'not' and i[0] != 'also':\n",
    "                print(i[0])\n",
    "#                 print(tokenizations.get_original_spans(tokens, requirement_text))\n",
    "                count+=1\n",
    "    return count/total_sentences\n",
    "#     print(\"Worliness index :\"+\"{:.2f}\".format(count/total_sentences))\n",
    "\n",
    "def get_ambiguity_index(sentences):\n",
    "    global len_sent\n",
    "    chunkGram = \"\"\"\n",
    "                    analytical: \n",
    "                        {<JJ><NN.*><NN.*>}\n",
    "                        \n",
    "                    coordination: \n",
    "                        {<JJ><NN.*><CC><NN.*>}\n",
    "                        \n",
    "                    PPAttachment: \n",
    "                        {<VB.*><DT>?<JJ>*<NN.*><IN><DT>?<JJ>*<NN.*>}\n",
    "            \"\"\"\n",
    "    count = 0\n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunkParser = nltk.RegexpParser(chunkGram)\n",
    "        result = chunkParser.parse(tagged)\n",
    "        \n",
    "        chunked_terms = []\n",
    "        for e in result.subtrees(filter=lambda t: t.label() == 'analytical' or t.label() == 'coordination' or t.label() == 'PPAttachment'):\n",
    "            if isinstance(e, tuple):\n",
    "                chunked_terms.append([ e[0] ])\n",
    "            else:\n",
    "                chunked_terms.append([w for w, t in e])\n",
    "                count+=1\n",
    "                print(chunked_terms)\n",
    "    return count/len_sent\n",
    "#     print(\"Ambiguity index :\"+\"{:.2f}\".format(count/len(sentences)))\n",
    "        \n",
    "\n",
    "def get_vagueness_index(sentences):\n",
    "    dictionary = []\n",
    "    global len_sent\n",
    "    with open(\"Vagueness_Dictionary\") as f:\n",
    "        dictionary = f.read().splitlines()\n",
    "    count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for token in dictionary:\n",
    "            if token in sent:\n",
    "                print(token)\n",
    "                count+=1\n",
    "    #modification required for considering POS tags\n",
    "    return count/len_sent\n",
    "#     print(\"Vagueness index :\"+\"{:.2f}\".format(count/len(sentences)))\n",
    "\n",
    "def get_readability_index(text):\n",
    "    \n",
    "    nc = len(text) \n",
    "    words = word_tokenize(text)\n",
    "    nw = len(words)\n",
    "    sentences = sent_tokenize(text)\n",
    "    ns = len(sentences)\n",
    "    \n",
    "    #NC is the number of character in the text, NW the number of words, and NS the number of sentences\n",
    "    \n",
    "    ig = 89 - 10*nc/nw + 300*ns/nw          #Gulpease Index IG\n",
    "    \n",
    "    chunkGram = r\"\"\"NP: \n",
    "                        {<DT>?<JJ>*<NN>+}\n",
    "                        \n",
    "                    VP: \n",
    "                        {<PRP>?<VB|VBD|VBZ|VBG>*<RB|RBR>?}\n",
    "                    \n",
    "                    ADJP:\n",
    "                        {<RB|RBR>?<JJ>+}\n",
    "                    \n",
    "                    ABVP:\n",
    "                        {<RB|RBR><RB|RBR>+}\n",
    "                    \n",
    "                    PP:\n",
    "                        {<IN><DT>*<NN>}\n",
    "    \"\"\"\n",
    "    nk = 0         #number of chunks\n",
    "    \n",
    "    #initializing each chunk phrase counter\n",
    "    NP_count = 0\n",
    "    VP_count = 0\n",
    "    ADVP_count = 0\n",
    "    ADJP_count = 0\n",
    "    PP_count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        chunkParser = nltk.RegexpParser(chunkGram)\n",
    "        result = chunkParser.parse(tagged)\n",
    "        \n",
    "        NP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'NP')))\n",
    "        VP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'VP')))\n",
    "        ADVP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'ADVP')))\n",
    "        ADJP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'ADJP')))\n",
    "        PP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'PP')))\n",
    "        \n",
    "        nk += (NP_count+VP_count+ADVP_count+ADJP_count+PP_count)\n",
    "                       \n",
    "    ik = 0        #Chunk Index Ik\n",
    "    if nk > 1:\n",
    "        ik = 100/(nk/len(sentences) - 1)\n",
    "    else:\n",
    "        ik = 100\n",
    "    \n",
    "    it = 0        #Chunk Type Index IT\n",
    "    \n",
    "    weighted_count = (NP_count*0.2691 + VP_count*0.4454 + ADJP_count*0.0379 + ADVP_count*0.0317 + PP_count*0.2159)\n",
    "    \n",
    "    if nk > 1:\n",
    "        it = 544*weighted_count/nk\n",
    "    else:\n",
    "        it = 100*weighted_count/0.2468\n",
    "    \n",
    "    print(\"Ig :\",ig,\"Ik :\",ik,\"It :\",it)\n",
    "    \n",
    "    readability_index = (ig + ik + it)/3\n",
    "    return 1 - readability_index/100\n",
    "#     print(\"Complexity index :\"+\"{:.2f}\".format(1 - readability_index/100))\n",
    "    \n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "def get_understandability_index(text):\n",
    "    all_words = word_tokenize(text)\n",
    "    words = [w for w in all_words if w not in stop_words]\n",
    "#     ps = PorterStemmer()\n",
    "#     for i in range(len(words)):\n",
    "#         words[i] = ps.stem(words[i])\n",
    "    \n",
    "    word_dictionary = []\n",
    "    with open(\"Basic_english_words\") as f:\n",
    "        dictionary = f.read().splitlines()\n",
    "    nl = 0          #least used words\n",
    "    nb = len(all_words)-len(words)          #basic words\n",
    "    for word in words:\n",
    "        if word in word_dictionary:\n",
    "            nb+=1\n",
    "        else:\n",
    "            nl+=1\n",
    "    print(\"Nb:\",nb,\"Nl:\",nl)\n",
    "    iu = (nb + 0.5*nl)/len(all_words)\n",
    "    \n",
    "    return 1-iu\n",
    "\n",
    "def get_quality_metrics(text_str):\n",
    "#     text_str = read_file(\"Point Of Sale System-PS.txt\")\n",
    "    global len_sent\n",
    "    sentences = sent_tokenize(text_str)\n",
    "    if len_sent == 0:\n",
    "        len_sent = len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = \"\".join([j.lower() for j in sentences[i] if j not in string.punctuation])\n",
    "\n",
    "    result_metrics = []\n",
    "    #calling metrics functions\n",
    "    print(\"-------------Duplication Index-----------\")\n",
    "    result_metrics.append(get_duplication_index(sentences))\n",
    "    print(\"-------------Wordliness Index-----------\")\n",
    "    result_metrics.append(get_wordliness_index(text_str,len_sent))\n",
    "    print(\"-------------Ambiguity Index-----------\")\n",
    "    result_metrics.append(get_ambiguity_index(sentences))\n",
    "    print(\"-------------Vagueness Index-----------\")\n",
    "    result_metrics.append(get_vagueness_index(sentences))\n",
    "    print(\"-------------Readability Index-----------\")\n",
    "    result_metrics.append(get_readability_index(text_str))\n",
    "    print(\"-------------Understandability Index-----------\")\n",
    "    result_metrics.append(get_understandability_index(text_str))\n",
    "    \n",
    "    return result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Automated conversion to EARS/Rupp's template\")\n",
    "frame=Frame(root, width=1300, height=600)\n",
    "frame.pack()\n",
    "\n",
    "len_sent = 0\n",
    "\n",
    "text1 = Label(text=\"Requirement description:\")\n",
    "text1.place(x=10, y=10)\n",
    "\n",
    "tbox1 = Text(frame,font=('Times', 14))\n",
    "tbox1.place(x=10, y=40, height=150, width=300)\n",
    "\n",
    "text2 = Label(text=\"Requirement Textual Specification:\")\n",
    "text2.place(x=10, y=200)\n",
    "\n",
    "tbox2 = Text(frame,font=('Times', 14))\n",
    "tbox2.place(x=10, y=225, height=360, width=300)\n",
    "\n",
    "button1 = Button(frame, text=\"Convert Requirements\", command = getTextInput)\n",
    "button1.place(x=320, y=300, height=50, width=125)\n",
    "\n",
    "\n",
    "\n",
    "# outputtext = Text(frame)\n",
    "# outputtext.place(x=450, y = 40, height=540, width=500)\n",
    "\n",
    "text3 = Label(text=\"Converted Requirements to EARS/Rupp's template format:\")\n",
    "text3.place(x=450, y=10)\n",
    "\n",
    "outputtext = Listbox(frame,height=10, font=('Times', 14))\n",
    "outputtext.place(x=450, y = 40, height=360, width=800)\n",
    "\n",
    "text4 = Label(text=\"Quality metrics before conversion:\")\n",
    "text4.place(x=450, y=420)\n",
    "\n",
    "# outputtext1 = Listbox(frame,height=10, font=('Times', 14))\n",
    "outputtext1 = Text(frame,height=10, font=('Times', 14))\n",
    "outputtext1.place(x=450, y = 450, height=130, width=350)\n",
    "\n",
    "text5 = Label(text=\"Quality metrics after conversion:\")\n",
    "text5.place(x=900, y=420)\n",
    "\n",
    "# outputtext1 = Listbox(frame,height=10, font=('Times', 14))\n",
    "outputtext2 = Text(frame,height=10, font=('Times', 14))\n",
    "outputtext2.place(x=900, y = 450, height=130, width=350)\n",
    "\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextInput():\n",
    "    desc=tbox1.get(\"1.0\",\"end\")\n",
    "    req=tbox2.get(\"1.0\",\"end\")\n",
    "    template_list, start_char, end_char = convert_requirements(desc,req)\n",
    "    template_string = \"\"\n",
    "    \n",
    "    for i in range(len(template_list)):\n",
    "        outputtext.insert(i, str(i+1) + \". \" + template_list[i] + \"\\n\\n\")\n",
    "        template_string += template_list[i]\n",
    "    \n",
    "    outputtext.bind('<Double-1>', lambda event, arg1=start_char, arg2 = end_char: go(event, arg1, arg2))\n",
    "    \n",
    "    \n",
    "    text_str= desc + req\n",
    "    output_metrics = get_quality_metrics(text_str)\n",
    "    output_string = \"Duplication index: \"+\"{:.2f}\".format(output_metrics[0])+\"\\n\"+\"Wordliness index: \"+\"{:.2f}\".format(output_metrics[1])+\"\\n\"+\"Ambiguity index: \"+\"{:.2f}\".format(output_metrics[2])+\"\\n\"+\"Vagueness index: \"+\"{:.2f}\".format(output_metrics[3])+\"\\n\"+\"Redability index: \"+\"{:.2f}\".format(output_metrics[4])+\"\\n\"+\"Understandability index: \"+\"{:.2f}\".format(output_metrics[5])\n",
    "    outputtext1.insert(END, output_string)\n",
    "    \n",
    "    print(template_string)\n",
    "    \n",
    "    output_metrics1 = get_quality_metrics(template_string)\n",
    "    output_string1 = \"Duplication index: \"+\"{:.2f}\".format(output_metrics1[0])+\"\\n\"+\"Wordliness index: \"+\"{:.2f}\".format(output_metrics1[1])+\"\\n\"+\"Ambiguity index: \"+\"{:.2f}\".format(output_metrics1[2])+\"\\n\"+\"Vagueness index: \"+\"{:.2f}\".format(output_metrics1[3])+\"\\n\"+\"Redability index: \"+\"{:.2f}\".format(output_metrics1[4])+\"\\n\"+\"Understandability index: \"+\"{:.2f}\".format(output_metrics1[5])\n",
    "    outputtext2.insert(END, output_string1)\n",
    "    \n",
    "#     for i in range(len(template_list)):\n",
    "#         template_string += str(i+1) + \". \" + template_list[i] + \"\\n\\n\"\n",
    "#     outputtext.insert(END, template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(event,start_char,end_char):\n",
    "    cs = outputtext.curselection()\n",
    "    tbox2.tag_add(\"highlight\", \"1.0+\"+str(start_char[cs[0]])+\"c\", \"1.0+\"+str(end_char[cs[0]])+\"c\") \n",
    "    tbox2.tag_config(\"highlight\", background=\"yellow\", foreground=\"black\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
